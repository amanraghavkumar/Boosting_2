{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfa5c8d0",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3615b0",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a machine learning technique that belongs to the family of boosting algorithms. It is used for regression tasks, where the goal is to predict a continuous outcome. The algorithm builds an ensemble of weak learners (typically decision trees) sequentially, with each learner correcting the errors made by the previous ones. The updates are made in the direction of the negative gradient of a loss function, optimizing the model to minimize the overall prediction errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e45379e",
   "metadata": {},
   "source": [
    "Q2. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c539ad2",
   "metadata": {},
   "source": [
    "In the context of Gradient Boosting, a weak learner is a model that performs slightly better than random chance on a particular task. Decision trees with limited depth (stumps) are commonly used as weak learners in Gradient Boosting algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135a8e7e",
   "metadata": {},
   "source": [
    "Q3. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475d9560",
   "metadata": {},
   "source": [
    "The intuition behind Gradient Boosting is to sequentially improve the model's predictions by focusing on the mistakes made by the previous models. It does this by fitting a series of weak learners to the residuals (the differences between the actual and predicted values) of the preceding models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c38b45",
   "metadata": {},
   "source": [
    "Q4. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546a2262",
   "metadata": {},
   "source": [
    "Gradient Boosting builds an ensemble by adding weak learners sequentially. Each weak learner is fitted to the negative gradient of the loss function with respect to the current ensemble's predictions. The resulting model is a weighted sum of the weak learners, where each learner contributes to the overall prediction based on its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c1a08e",
   "metadata": {},
   "source": [
    "Q7. What are the steps involved in constructing the mathematical intuition of the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bed22c2",
   "metadata": {},
   "source": [
    "The mathematical intuition of Gradient Boosting involves the following steps:\n",
    "\n",
    "* Initialize the predictions with the mean (or another constant) of the target variable.\n",
    "* Compute the negative gradient of the loss function with respect to the current predictions.\n",
    "* Fit a weak learner to the negative gradient, capturing the patterns in the residuals.\n",
    "* Update the predictions by adding a fraction (learning rate) of the weak learner's predictions.\n",
    "* Repeat steps 2-4 for a predefined number of iterations (trees) or until convergence.\n",
    "* The final ensemble is the sum of all weak learners, each weighted by the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11719ddb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
